{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f8412c",
   "metadata": {},
   "source": [
    "# Miniproyecto 2 Redes Neuronales\n",
    "\n",
    "- **Profesor:** Francisco Perez Galarce\n",
    "- **Ayudante:** Yesenia Salinas\n",
    "\n",
    "**Integrante:**\n",
    "- Jorge Troncoso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5969dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerias a utilizar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc070b",
   "metadata": {},
   "source": [
    "# 1.1 Perceptron multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9502da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento y carga de datos de MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                 # Convertimos imágenes a tensores\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizamos a media 0 y varianza 1\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Crear dataloaders\n",
    "batch_size = 64  # Ajusta según la capacidad de GPU/CPU\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cde384",
   "metadata": {},
   "source": [
    "En el siguiente bloque de codigo se hara tanto la modificación de arquitectura como las modifaciones de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298be15d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3447279696742694\n",
      "Epoch [2/3], Loss: 0.1699446654121081\n",
      "Epoch [3/3], Loss: 0.12821243075355887\n",
      "Train Accuracy: 97.03%, Test Accuracy: 96.43%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.5335675369898478\n",
      "Epoch [2/3], Loss: 0.7393601686636607\n",
      "Epoch [3/3], Loss: 0.538245778520902\n",
      "Train Accuracy: 87.52%, Test Accuracy: 88.01%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.4049938795487086\n",
      "Epoch [2/3], Loss: 0.1965350634942452\n",
      "Epoch [3/3], Loss: 0.14589538647284112\n",
      "Train Accuracy: 96.79%, Test Accuracy: 96.22%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.217266143544515\n",
      "Epoch [2/3], Loss: 2.0094561297098794\n",
      "Epoch [3/3], Loss: 1.7560285143534342\n",
      "Train Accuracy: 71.45%, Test Accuracy: 72.86%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3079059141258399\n",
      "Epoch [2/3], Loss: 0.14796543912688892\n",
      "Epoch [3/3], Loss: 0.10981847726752361\n",
      "Train Accuracy: 96.81%, Test Accuracy: 96.30%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.0568165318171183\n",
      "Epoch [2/3], Loss: 1.1895438321431477\n",
      "Epoch [3/3], Loss: 0.6761494761943817\n",
      "Train Accuracy: 85.61%, Test Accuracy: 86.32%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.40494417422910534\n",
      "Epoch [2/3], Loss: 0.15746312631318968\n",
      "Epoch [3/3], Loss: 0.11454615754137437\n",
      "Train Accuracy: 97.56%, Test Accuracy: 96.98%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.304174777984619\n",
      "Epoch [2/3], Loss: 2.2959259001413983\n",
      "Epoch [3/3], Loss: 2.2906442822774253\n",
      "Train Accuracy: 11.33%, Test Accuracy: 11.48%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3035767988617222\n",
      "Epoch [2/3], Loss: 0.1487840723304699\n",
      "Epoch [3/3], Loss: 0.11495558643589417\n",
      "Train Accuracy: 97.28%, Test Accuracy: 96.73%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2560335353851317\n",
      "Epoch [2/3], Loss: 1.9857461044947307\n",
      "Epoch [3/3], Loss: 1.1750160808563233\n",
      "Train Accuracy: 79.30%, Test Accuracy: 79.88%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.42746190211474894\n",
      "Epoch [2/3], Loss: 0.16158872569153707\n",
      "Epoch [3/3], Loss: 0.1225930577300489\n",
      "Train Accuracy: 96.80%, Test Accuracy: 96.10%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3067314741770426\n",
      "Epoch [2/3], Loss: 2.301244281133016\n",
      "Epoch [3/3], Loss: 2.301096531041463\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.42248622870743274\n",
      "Epoch [2/3], Loss: 0.34188596717814607\n",
      "Epoch [3/3], Loss: 0.32211744655023017\n",
      "Train Accuracy: 92.81%, Test Accuracy: 92.74%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.5700919307669003\n",
      "Epoch [2/3], Loss: 0.31115918449958163\n",
      "Epoch [3/3], Loss: 0.2640207984258731\n",
      "Train Accuracy: 92.96%, Test Accuracy: 92.79%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.5550715933005015\n",
      "Epoch [2/3], Loss: 0.48210645522276563\n",
      "Epoch [3/3], Loss: 0.45514814260403313\n",
      "Train Accuracy: 85.51%, Test Accuracy: 85.94%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.3087553638617198\n",
      "Epoch [2/3], Loss: 0.5536598283052444\n",
      "Epoch [3/3], Loss: 0.42357916610240937\n",
      "Train Accuracy: 89.44%, Test Accuracy: 89.96%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.44225204476118085\n",
      "Epoch [2/3], Loss: 0.3284375795776645\n",
      "Epoch [3/3], Loss: 0.31304923750211794\n",
      "Train Accuracy: 90.48%, Test Accuracy: 90.86%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.6915874194562435\n",
      "Epoch [2/3], Loss: 0.3092312136809031\n",
      "Epoch [3/3], Loss: 0.2532151914099852\n",
      "Train Accuracy: 93.40%, Test Accuracy: 93.44%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.7365374496459961\n",
      "Epoch [2/3], Loss: 0.5206813677469889\n",
      "Epoch [3/3], Loss: 0.47621908397475876\n",
      "Train Accuracy: 87.36%, Test Accuracy: 87.09%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2729768065134683\n",
      "Epoch [2/3], Loss: 1.9613723939259846\n",
      "Epoch [3/3], Loss: 1.1898291874885558\n",
      "Train Accuracy: 73.93%, Test Accuracy: 74.30%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.48652776128252345\n",
      "Epoch [2/3], Loss: 0.37396025804479915\n",
      "Epoch [3/3], Loss: 0.3302409612228473\n",
      "Train Accuracy: 91.46%, Test Accuracy: 91.38%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.8895409260233244\n",
      "Epoch [2/3], Loss: 0.31659157854914666\n",
      "Epoch [3/3], Loss: 0.2443584785014391\n",
      "Train Accuracy: 93.67%, Test Accuracy: 93.63%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 2.3033239120483397\n",
      "Epoch [2/3], Loss: 2.3026454249064128\n",
      "Epoch [3/3], Loss: 2.3030910783131917\n",
      "Train Accuracy: 9.93%, Test Accuracy: 10.32%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3035496841430665\n",
      "Epoch [2/3], Loss: 2.301682694498698\n",
      "Epoch [3/3], Loss: 2.2993578046162924\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.38540839714440966\n",
      "Epoch [2/3], Loss: 0.2007106315277056\n",
      "Epoch [3/3], Loss: 0.14557328371247694\n",
      "Train Accuracy: 95.81%, Test Accuracy: 95.53%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.9116417595318385\n",
      "Epoch [2/3], Loss: 1.1937532811276694\n",
      "Epoch [3/3], Loss: 0.8201434551271548\n",
      "Train Accuracy: 84.25%, Test Accuracy: 85.02%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.48050248352036296\n",
      "Epoch [2/3], Loss: 0.2251380700062015\n",
      "Epoch [3/3], Loss: 0.17178049441307847\n",
      "Train Accuracy: 95.80%, Test Accuracy: 95.52%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.258503119066072\n",
      "Epoch [2/3], Loss: 2.1693951892954453\n",
      "Epoch [3/3], Loss: 2.073139704112559\n",
      "Train Accuracy: 61.42%, Test Accuracy: 62.88%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3375118201229173\n",
      "Epoch [2/3], Loss: 0.14655184533907725\n",
      "Epoch [3/3], Loss: 0.10850734239134358\n",
      "Train Accuracy: 97.04%, Test Accuracy: 96.42%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2135461492579123\n",
      "Epoch [2/3], Loss: 1.9209799033238182\n",
      "Epoch [3/3], Loss: 1.3997930284502156\n",
      "Train Accuracy: 77.11%, Test Accuracy: 78.03%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.49200111847601213\n",
      "Epoch [2/3], Loss: 0.17491909791864374\n",
      "Epoch [3/3], Loss: 0.12218713255396593\n",
      "Train Accuracy: 97.00%, Test Accuracy: 96.22%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3036901035796857\n",
      "Epoch [2/3], Loss: 2.2974442952731526\n",
      "Epoch [3/3], Loss: 2.295168396506482\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.33267770994748513\n",
      "Epoch [2/3], Loss: 0.14468885847786342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Loss: 0.11270095981401342\n",
      "Train Accuracy: 97.17%, Test Accuracy: 96.65%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.281759505587092\n",
      "Epoch [2/3], Loss: 2.2175865320762846\n",
      "Epoch [3/3], Loss: 2.0855780517114506\n",
      "Train Accuracy: 54.45%, Test Accuracy: 55.26%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.5071390755295849\n",
      "Epoch [2/3], Loss: 0.1672256277925742\n",
      "Epoch [3/3], Loss: 0.11762746465283194\n",
      "Train Accuracy: 97.33%, Test Accuracy: 96.54%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3065902208214375\n",
      "Epoch [2/3], Loss: 2.3011423293461424\n",
      "Epoch [3/3], Loss: 2.301063333493052\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.4244051267152656\n",
      "Epoch [2/3], Loss: 0.27683216802426364\n",
      "Epoch [3/3], Loss: 0.25225650303677394\n",
      "Train Accuracy: 92.81%, Test Accuracy: 92.40%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.7502501797892137\n",
      "Epoch [2/3], Loss: 0.36960331231419213\n",
      "Epoch [3/3], Loss: 0.32315585802771896\n",
      "Train Accuracy: 91.10%, Test Accuracy: 91.33%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.4810974602220155\n",
      "Epoch [2/3], Loss: 0.3666122185824904\n",
      "Epoch [3/3], Loss: 0.3308877024426262\n",
      "Train Accuracy: 89.65%, Test Accuracy: 89.83%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.7651780947947553\n",
      "Epoch [2/3], Loss: 0.9039194655062547\n",
      "Epoch [3/3], Loss: 0.6151133805576926\n",
      "Train Accuracy: 87.13%, Test Accuracy: 87.72%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.40334341323563155\n",
      "Epoch [2/3], Loss: 0.26744142689430384\n",
      "Epoch [3/3], Loss: 0.2539779929412422\n",
      "Train Accuracy: 91.32%, Test Accuracy: 91.07%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.9812655999366917\n",
      "Epoch [2/3], Loss: 0.3786165745082949\n",
      "Epoch [3/3], Loss: 0.31947228032102715\n",
      "Train Accuracy: 91.14%, Test Accuracy: 91.50%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.614411215045686\n",
      "Epoch [2/3], Loss: 0.3999735041023063\n",
      "Epoch [3/3], Loss: 0.35762457961021965\n",
      "Train Accuracy: 87.40%, Test Accuracy: 87.57%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2922369861907796\n",
      "Epoch [2/3], Loss: 2.2559181202703447\n",
      "Epoch [3/3], Loss: 2.146525289839519\n",
      "Train Accuracy: 43.48%, Test Accuracy: 44.21%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.49230037686397143\n",
      "Epoch [2/3], Loss: 0.2633975143911742\n",
      "Epoch [3/3], Loss: 0.24073136519235588\n",
      "Train Accuracy: 94.89%, Test Accuracy: 94.23%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.3506683846717196\n",
      "Epoch [2/3], Loss: 0.4123342086447836\n",
      "Epoch [3/3], Loss: 0.3282964086529416\n",
      "Train Accuracy: 91.33%, Test Accuracy: 91.55%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 1.0292762904596735\n",
      "Epoch [2/3], Loss: 0.5731181918716888\n",
      "Epoch [3/3], Loss: 0.5178883922284346\n",
      "Train Accuracy: 80.44%, Test Accuracy: 80.94%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3025384561847777\n",
      "Epoch [2/3], Loss: 2.3014930662061612\n",
      "Epoch [3/3], Loss: 2.3006053090349696\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "    batch_size  learning_rate    hidden_layers activation optimizer  \\\n",
      "6           32          0.001       [256, 128]    Sigmoid      Adam   \n",
      "8           32          0.001  [512, 256, 128]       ReLU      Adam   \n",
      "32          64          0.001  [512, 256, 128]       ReLU      Adam   \n",
      "34          64          0.001  [512, 256, 128]    Sigmoid      Adam   \n",
      "0           32          0.001            [128]       ReLU      Adam   \n",
      "28          64          0.001       [256, 128]       ReLU      Adam   \n",
      "4           32          0.001       [256, 128]       ReLU      Adam   \n",
      "2           32          0.001            [128]    Sigmoid      Adam   \n",
      "30          64          0.001       [256, 128]    Sigmoid      Adam   \n",
      "10          32          0.001  [512, 256, 128]    Sigmoid      Adam   \n",
      "24          64          0.001            [128]       ReLU      Adam   \n",
      "26          64          0.001            [128]    Sigmoid      Adam   \n",
      "44          64          0.010  [512, 256, 128]       ReLU      Adam   \n",
      "21          32          0.010  [512, 256, 128]       ReLU       SGD   \n",
      "17          32          0.010       [256, 128]       ReLU       SGD   \n",
      "13          32          0.010            [128]       ReLU       SGD   \n",
      "12          32          0.010            [128]       ReLU      Adam   \n",
      "36          64          0.010            [128]       ReLU      Adam   \n",
      "45          64          0.010  [512, 256, 128]       ReLU       SGD   \n",
      "41          64          0.010       [256, 128]       ReLU       SGD   \n",
      "20          32          0.010  [512, 256, 128]       ReLU      Adam   \n",
      "37          64          0.010            [128]       ReLU       SGD   \n",
      "40          64          0.010       [256, 128]       ReLU      Adam   \n",
      "16          32          0.010       [256, 128]       ReLU      Adam   \n",
      "15          32          0.010            [128]    Sigmoid       SGD   \n",
      "38          64          0.010            [128]    Sigmoid      Adam   \n",
      "1           32          0.001            [128]       ReLU       SGD   \n",
      "39          64          0.010            [128]    Sigmoid       SGD   \n",
      "42          64          0.010       [256, 128]    Sigmoid      Adam   \n",
      "18          32          0.010       [256, 128]    Sigmoid      Adam   \n",
      "5           32          0.001       [256, 128]       ReLU       SGD   \n",
      "14          32          0.010            [128]    Sigmoid      Adam   \n",
      "25          64          0.001            [128]       ReLU       SGD   \n",
      "46          64          0.010  [512, 256, 128]    Sigmoid      Adam   \n",
      "9           32          0.001  [512, 256, 128]       ReLU       SGD   \n",
      "29          64          0.001       [256, 128]       ReLU       SGD   \n",
      "19          32          0.010       [256, 128]    Sigmoid       SGD   \n",
      "3           32          0.001            [128]    Sigmoid       SGD   \n",
      "27          64          0.001            [128]    Sigmoid       SGD   \n",
      "33          64          0.001  [512, 256, 128]       ReLU       SGD   \n",
      "43          64          0.010       [256, 128]    Sigmoid       SGD   \n",
      "7           32          0.001       [256, 128]    Sigmoid       SGD   \n",
      "11          32          0.001  [512, 256, 128]    Sigmoid       SGD   \n",
      "35          64          0.001  [512, 256, 128]    Sigmoid       SGD   \n",
      "31          64          0.001       [256, 128]    Sigmoid       SGD   \n",
      "23          32          0.010  [512, 256, 128]    Sigmoid       SGD   \n",
      "47          64          0.010  [512, 256, 128]    Sigmoid       SGD   \n",
      "22          32          0.010  [512, 256, 128]    Sigmoid      Adam   \n",
      "\n",
      "    train_accuracy  test_accuracy  \n",
      "6         0.975600         0.9698  \n",
      "8         0.972783         0.9673  \n",
      "32        0.971750         0.9665  \n",
      "34        0.973333         0.9654  \n",
      "0         0.970333         0.9643  \n",
      "28        0.970433         0.9642  \n",
      "4         0.968100         0.9630  \n",
      "2         0.967867         0.9622  \n",
      "30        0.970017         0.9622  \n",
      "10        0.968050         0.9610  \n",
      "24        0.958150         0.9553  \n",
      "26        0.957950         0.9552  \n",
      "44        0.948917         0.9423  \n",
      "21        0.936700         0.9363  \n",
      "17        0.934000         0.9344  \n",
      "13        0.929567         0.9279  \n",
      "12        0.928083         0.9274  \n",
      "36        0.928100         0.9240  \n",
      "45        0.913267         0.9155  \n",
      "41        0.911450         0.9150  \n",
      "20        0.914567         0.9138  \n",
      "37        0.910967         0.9133  \n",
      "40        0.913167         0.9107  \n",
      "16        0.904783         0.9086  \n",
      "15        0.894383         0.8996  \n",
      "38        0.896500         0.8983  \n",
      "1         0.875167         0.8801  \n",
      "39        0.871317         0.8772  \n",
      "42        0.874017         0.8757  \n",
      "18        0.873617         0.8709  \n",
      "5         0.856067         0.8632  \n",
      "14        0.855117         0.8594  \n",
      "25        0.842533         0.8502  \n",
      "46        0.804350         0.8094  \n",
      "9         0.793033         0.7988  \n",
      "29        0.771067         0.7803  \n",
      "19        0.739283         0.7430  \n",
      "3         0.714500         0.7286  \n",
      "27        0.614167         0.6288  \n",
      "33        0.544517         0.5526  \n",
      "43        0.434750         0.4421  \n",
      "7         0.113267         0.1148  \n",
      "11        0.112367         0.1135  \n",
      "35        0.112367         0.1135  \n",
      "31        0.112367         0.1135  \n",
      "23        0.112367         0.1135  \n",
      "47        0.112367         0.1135  \n",
      "22        0.099300         0.1032  \n"
     ]
    }
   ],
   "source": [
    "# Función para construir un MLP flexible\n",
    "# Solo se usaran 3 epocas como menciono el profesor para que la ejecución sea mas rapida\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation_function):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(activation_function())\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Aplanar las imágenes\n",
    "        return self.network(x)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, train_loader, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Experimentación\n",
    "results = []\n",
    "batch_sizes = [32, 64]\n",
    "learning_rates = [0.001, 0.01]\n",
    "hidden_layers = [[128], [256, 128], [512, 256, 128]]\n",
    "activation_functions = [nn.ReLU, nn.Sigmoid]\n",
    "optimizers = [optim.Adam, optim.SGD]\n",
    "\n",
    "input_size = 28 * 28  # Tamaño de entrada\n",
    "output_size = 10  # 10 clases de dígitos\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for lr in learning_rates:\n",
    "        for hidden_sizes in hidden_layers:\n",
    "            for activation_function in activation_functions:\n",
    "                for opt_func in optimizers:\n",
    "                    print(f\"Testing: batch_size={batch_size}, lr={lr}, hidden_sizes={hidden_sizes}, activation={activation_function.__name__}, optimizer={opt_func.__name__}\")\n",
    "                    # Crear modelo\n",
    "                    model = MLP(input_size, hidden_sizes, output_size, activation_function)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt_func(model.parameters(), lr=lr)\n",
    "                    # Entrenar modelo\n",
    "                    train_model(model, train_loader, optimizer, criterion, num_epochs=3)\n",
    "                    # Evaluar modelo\n",
    "                    train_acc = evaluate_model(model, train_loader)\n",
    "                    test_acc = evaluate_model(model, test_loader)\n",
    "                    print(f\"Train Accuracy: {train_acc * 100:.2f}%, Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "                    # Guardar resultados\n",
    "                    results.append({\n",
    "                        'batch_size': batch_size,\n",
    "                        'learning_rate': lr,\n",
    "                        'hidden_layers': hidden_sizes,\n",
    "                        'activation': activation_function.__name__,\n",
    "                        'optimizer': opt_func.__name__,\n",
    "                        'train_accuracy': train_acc,\n",
    "                        'test_accuracy': test_acc\n",
    "                    })\n",
    "\n",
    "# Mostrar resultados ordenados por mejor accuracy en test\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='test_accuracy', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf195f3e",
   "metadata": {},
   "source": [
    "## **Informe Detallado de los Experimentos** (Tanto de la arquitectura como del entrenamiento)\n",
    "\n",
    "## **Objetivo de los Experimentos**\n",
    "El propósito de estos experimentos fue evaluar cómo diferentes configuraciones arquitectónicas y estrategias de entrenamiento impactan el desempeño de un Perceptrón Multicapa (MLP) entrenado para clasificar dígitos del dataset MNIST. El análisis se llevó a cabo con dos enfoques principales:\n",
    "\n",
    "1. **Arquitectura del Modelo:**\n",
    "   - Variar el número de capas ocultas y neuronas.\n",
    "   - Cambiar la función de activación.\n",
    "2. **Estrategia de Entrenamiento:**\n",
    "   - Probar diferentes optimizadores y tasas de aprendizaje.\n",
    "   - Evaluar el impacto del tamaño del batch.\n",
    "\n",
    "---\n",
    "\n",
    "## **Parte 1: Modificaciones en la Arquitectura**\n",
    "\n",
    "### **Configuraciones Probadas**\n",
    "Se probaron 3 configuraciones de capas ocultas: \n",
    "- `[128]`\n",
    "- `[256, 128]`\n",
    "- `[512, 256, 128]`\n",
    "\n",
    "Cada configuración se probó con dos funciones de activación: \n",
    "- `ReLU` (Rectified Linear Unit).\n",
    "- `Sigmoid`.\n",
    "\n",
    "### **Resultados Resumidos**\n",
    "\n",
    "| Arquitectura         | Activación | Optimizador | Train Accuracy | Test Accuracy | Observaciones                                          |\n",
    "|----------------------|------------|-------------|----------------|---------------|-------------------------------------------------------|\n",
    "| `[256, 128]`         | `Sigmoid`  | `Adam`      | 97.56%         | **96.98%**    | Excelente generalización con configuraciones intermedias. |\n",
    "| `[512, 256, 128]`    | `ReLU`     | `Adam`      | 97.28%         | 96.73%        | Profundidad proporciona generalización sólida.         |\n",
    "| `[128]`              | `ReLU`     | `Adam`      | 97.03%         | 96.43%        | Arquitectura más simple con buen rendimiento.          |\n",
    "| `[256, 128]`         | `ReLU`     | `Adam`      | 96.81%         | 96.30%        | Buen rendimiento general, menor que configuraciones más profundas. |\n",
    "| `[128]`              | `Sigmoid`  | `Adam`      | 96.79%         | 96.22%        | `Sigmoid` limita el rendimiento en configuraciones simples. |\n",
    "| `[512, 256, 128]`    | `Sigmoid`  | `Adam`      | 96.80%         | 96.10%        | Menor precisión comparada con `ReLU`.                  |\n",
    "\n",
    "### **Análisis y Conclusión**\n",
    "\n",
    "1. **Función de Activación:**\n",
    "   - `ReLU` mostró mejor desempeño en general, especialmente en configuraciones profundas.\n",
    "   - `Sigmoid` tiene problemas con arquitecturas más complejas debido al gradiente desvanecido.\n",
    "\n",
    "2. **Arquitectura:**\n",
    "   - Las arquitecturas intermedias (`[256, 128]`) logran el mejor equilibrio entre rendimiento y costo computacional.\n",
    "   - Redes más profundas como `[512, 256, 128]` ofrecen mayor capacidad de modelado pero requieren más tiempo de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## **Parte 2: Modificaciones en el Entrenamiento**\n",
    "\n",
    "### **Configuraciones Probadas**\n",
    "- **Tamaño del batch:** `32` y `64`.\n",
    "- **Learning rate (lr):** `0.001` y `0.01`.\n",
    "- **Optimizadores:** `Adam` y `SGD`.\n",
    "\n",
    "### **Resultados Resumidos**\n",
    "\n",
    "| Batch Size | LR    | Optimizador | Train Accuracy | Test Accuracy | Observaciones                                    |\n",
    "|------------|-------|-------------|----------------|---------------|-------------------------------------------------|\n",
    "| `32`       | 0.001 | `Adam`      | 97.56%         | **96.98%**    | Estabilidad y buena generalización.             |\n",
    "| `64`       | 0.001 | `Adam`      | 97.33%         | 96.54%        | Precisión sólida con menor costo computacional. |\n",
    "| `32`       | 0.001 | `SGD`       | 87.52%         | 88.01%        | Lento en converger, menor precisión general.    |\n",
    "| `32`       | 0.01  | `Adam`      | 92.81%         | 92.74%        | Learning rate alto afecta la generalización.    |\n",
    "| `64`       | 0.01  | `SGD`       | 91.14%         | 91.50%        | `SGD` mejora con menor batch size.              |\n",
    "\n",
    "### **Análisis y Conclusión**\n",
    "\n",
    "1. **Tasa de Aprendizaje:**\n",
    "   - `lr=0.001` fue óptimo, especialmente con `Adam`.\n",
    "   - `lr=0.01` mostró menor estabilidad en la generalización.\n",
    "\n",
    "2. **Optimizador:**\n",
    "   - `Adam` superó consistentemente a `SGD` en precisión y convergencia.\n",
    "   - `SGD` es competitivo con tasas de aprendizaje adecuadas pero requiere más ajuste fino.\n",
    "\n",
    "3. **Tamaño del Batch:**\n",
    "   - Batch size de `32` proporcionó mejor generalización que `64`, aunque a mayor costo computacional.\n",
    "\n",
    "---\n",
    "\n",
    "## **Selección del Modelo Final**\n",
    "\n",
    "### **Modelo Seleccionado:**\n",
    "- **Arquitectura:** `[256, 128]` con `Sigmoid`.\n",
    "- **Entrenamiento:**\n",
    "  - **Batch size:** `32`.\n",
    "  - **Learning rate:** `0.001`.\n",
    "  - **Optimizador:** `Adam`.\n",
    "\n",
    "### **Rendimiento:**\n",
    "- **Train Accuracy:** 97.56%.\n",
    "- **Test Accuracy:** 96.98%.\n",
    "\n",
    "### **Justificación:**\n",
    "- La configuración seleccionada equilibra profundidad y estabilidad de entrenamiento.\n",
    "- Proporciona el mejor desempeño en términos de precisión en pruebas.\n",
    "- Es computacionalmente eficiente.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusión General**\n",
    "\n",
    "1. **Arquitectura:**\n",
    "   - Las configuraciones intermedias (`[256, 128]`) mostraron ser ideales para este problema, combinando simplicidad y precisión.\n",
    "   - Configuraciones más simples (`[128]`) son útiles en escenarios con limitaciones computacionales.\n",
    "\n",
    "2. **Estrategia de Entrenamiento:**\n",
    "   - El optimizador `Adam` con `lr=0.001` y batch size de `32` fue la mejor combinación para alcanzar alta precisión.\n",
    "\n",
    "3. **Recomendaciones Futuras:**\n",
    "   - Extender el número de épocas para confirmar estabilidad en redes profundas.\n",
    "   - Probar técnicas de regularización como Dropout o Batch Normalization.\n",
    "   - Aplicar la configuración seleccionada en datasets más complejos para evaluar generalización.\n",
    "\n",
    "---\n",
    "\n",
    "## **Apreciaciones Extras**\n",
    "- **Impacto del Gradiente Desvanecido:** `Sigmoid` funciona bien en arquitecturas simples pero es menos eficiente en configuraciones profundas.\n",
    "- **Implicaciones Prácticas:** En dispositivos con restricciones, usar configuraciones simples como `[128]` puede ser una solución viable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d917a877",
   "metadata": {},
   "source": [
    "# 1.2 Redes convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c355a89",
   "metadata": {},
   "source": [
    "En el siguiente bloque de codigo se hara tanto la modificación de arquitectura como las modifaciones de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0f9b515",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: optimizer=Adam, lr=0.001, filters=[16, 32], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 0.2851\n",
      "Epoch [2/3], Loss: 0.0774\n",
      "Epoch [3/3], Loss: 0.0569\n",
      "Train Accuracy: 99.15%, Test Accuracy: 98.87%\n",
      "Testing: optimizer=Adam, lr=0.001, filters=[16, 32], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 0.3583\n",
      "Epoch [2/3], Loss: 0.1256\n",
      "Epoch [3/3], Loss: 0.0962\n",
      "Train Accuracy: 98.80%, Test Accuracy: 98.60%\n",
      "Testing: optimizer=Adam, lr=0.001, filters=[16, 32], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 0.2237\n",
      "Epoch [2/3], Loss: 0.0620\n",
      "Epoch [3/3], Loss: 0.0460\n",
      "Train Accuracy: 99.18%, Test Accuracy: 98.79%\n",
      "Testing: optimizer=Adam, lr=0.001, filters=[16, 32], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 0.2551\n",
      "Epoch [2/3], Loss: 0.0788\n",
      "Epoch [3/3], Loss: 0.0618\n",
      "Train Accuracy: 99.16%, Test Accuracy: 98.91%\n",
      "Testing: optimizer=Adam, lr=0.001, filters=[32, 64], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 0.2282\n",
      "Epoch [2/3], Loss: 0.0682\n",
      "Epoch [3/3], Loss: 0.0485\n",
      "Train Accuracy: 99.28%, Test Accuracy: 98.93%\n",
      "Testing: optimizer=Adam, lr=0.001, filters=[32, 64], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 0.3607\n",
      "Epoch [2/3], Loss: 0.1328\n",
      "Epoch [3/3], Loss: 0.0959\n",
      "Train Accuracy: 99.00%, Test Accuracy: 98.64%\n",
      "Testing: optimizer=Adam, lr=0.001, filters=[32, 64], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 0.1942\n",
      "Epoch [2/3], Loss: 0.0570\n",
      "Epoch [3/3], Loss: 0.0419\n",
      "Train Accuracy: 99.28%, Test Accuracy: 99.04%\n",
      "Testing: optimizer=Adam, lr=0.001, filters=[32, 64], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 0.2208\n",
      "Epoch [2/3], Loss: 0.0692\n",
      "Epoch [3/3], Loss: 0.0533\n",
      "Train Accuracy: 99.29%, Test Accuracy: 99.05%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[16, 32], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 0.2793\n",
      "Epoch [2/3], Loss: 0.1533\n",
      "Epoch [3/3], Loss: 0.1388\n",
      "Train Accuracy: 97.75%, Test Accuracy: 97.80%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[16, 32], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 0.3031\n",
      "Epoch [2/3], Loss: 0.1768\n",
      "Epoch [3/3], Loss: 0.1674\n",
      "Train Accuracy: 96.95%, Test Accuracy: 97.24%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[16, 32], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 2.3051\n",
      "Epoch [2/3], Loss: 2.3021\n",
      "Epoch [3/3], Loss: 2.3022\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[16, 32], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 2.3044\n",
      "Epoch [2/3], Loss: 2.3019\n",
      "Epoch [3/3], Loss: 2.3021\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[32, 64], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 2.3039\n",
      "Epoch [2/3], Loss: 2.3021\n",
      "Epoch [3/3], Loss: 2.3023\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[32, 64], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 0.3522\n",
      "Epoch [2/3], Loss: 0.2096\n",
      "Epoch [3/3], Loss: 0.1907\n",
      "Train Accuracy: 96.58%, Test Accuracy: 96.36%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[32, 64], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 0.2718\n",
      "Epoch [2/3], Loss: 0.1619\n",
      "Epoch [3/3], Loss: 0.1439\n",
      "Train Accuracy: 97.48%, Test Accuracy: 97.05%\n",
      "Testing: optimizer=Adam, lr=0.01, filters=[32, 64], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 0.3792\n",
      "Epoch [2/3], Loss: 0.2147\n",
      "Epoch [3/3], Loss: 0.1972\n",
      "Train Accuracy: 97.18%, Test Accuracy: 97.20%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[16, 32], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 2.3021\n",
      "Epoch [2/3], Loss: 2.2970\n",
      "Epoch [3/3], Loss: 2.2914\n",
      "Train Accuracy: 15.54%, Test Accuracy: 15.65%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[16, 32], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 2.3019\n",
      "Epoch [2/3], Loss: 2.2969\n",
      "Epoch [3/3], Loss: 2.2909\n",
      "Train Accuracy: 20.79%, Test Accuracy: 21.08%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[16, 32], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 2.2997\n",
      "Epoch [2/3], Loss: 2.2906\n",
      "Epoch [3/3], Loss: 2.2786\n",
      "Train Accuracy: 19.16%, Test Accuracy: 19.23%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[16, 32], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 2.2986\n",
      "Epoch [2/3], Loss: 2.2882\n",
      "Epoch [3/3], Loss: 2.2742\n",
      "Train Accuracy: 28.56%, Test Accuracy: 29.02%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[32, 64], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 2.2953\n",
      "Epoch [2/3], Loss: 2.2739\n",
      "Epoch [3/3], Loss: 2.2325\n",
      "Train Accuracy: 30.55%, Test Accuracy: 31.33%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[32, 64], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 2.2973\n",
      "Epoch [2/3], Loss: 2.2834\n",
      "Epoch [3/3], Loss: 2.2621\n",
      "Train Accuracy: 38.58%, Test Accuracy: 38.89%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[32, 64], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 2.2979\n",
      "Epoch [2/3], Loss: 2.2852\n",
      "Epoch [3/3], Loss: 2.2661\n",
      "Train Accuracy: 35.28%, Test Accuracy: 37.01%\n",
      "Testing: optimizer=SGD, lr=0.001, filters=[32, 64], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 2.2938\n",
      "Epoch [2/3], Loss: 2.2673\n",
      "Epoch [3/3], Loss: 2.2177\n",
      "Train Accuracy: 61.76%, Test Accuracy: 62.21%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[16, 32], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 1.6644\n",
      "Epoch [2/3], Loss: 0.3831\n",
      "Epoch [3/3], Loss: 0.2419\n",
      "Train Accuracy: 94.94%, Test Accuracy: 95.16%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[16, 32], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 1.9871\n",
      "Epoch [2/3], Loss: 0.5478\n",
      "Epoch [3/3], Loss: 0.3005\n",
      "Train Accuracy: 95.13%, Test Accuracy: 95.43%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[16, 32], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 2.0266\n",
      "Epoch [2/3], Loss: 0.4155\n",
      "Epoch [3/3], Loss: 0.1966\n",
      "Train Accuracy: 95.95%, Test Accuracy: 96.29%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[16, 32], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 1.8317\n",
      "Epoch [2/3], Loss: 0.4103\n",
      "Epoch [3/3], Loss: 0.2299\n",
      "Train Accuracy: 95.26%, Test Accuracy: 95.69%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[32, 64], fc_neurons=[128, 64], dropout=0.25\n",
      "Epoch [1/3], Loss: 1.4603\n",
      "Epoch [2/3], Loss: 0.3112\n",
      "Epoch [3/3], Loss: 0.1877\n",
      "Train Accuracy: 96.18%, Test Accuracy: 96.49%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[32, 64], fc_neurons=[128, 64], dropout=0.5\n",
      "Epoch [1/3], Loss: 1.6086\n",
      "Epoch [2/3], Loss: 0.3600\n",
      "Epoch [3/3], Loss: 0.2113\n",
      "Train Accuracy: 96.26%, Test Accuracy: 96.58%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[32, 64], fc_neurons=[256, 128], dropout=0.25\n",
      "Epoch [1/3], Loss: 1.5149\n",
      "Epoch [2/3], Loss: 0.3032\n",
      "Epoch [3/3], Loss: 0.1802\n",
      "Train Accuracy: 95.98%, Test Accuracy: 96.31%\n",
      "Testing: optimizer=SGD, lr=0.01, filters=[32, 64], fc_neurons=[256, 128], dropout=0.5\n",
      "Epoch [1/3], Loss: 1.3607\n",
      "Epoch [2/3], Loss: 0.3261\n",
      "Epoch [3/3], Loss: 0.1970\n",
      "Train Accuracy: 96.29%, Test Accuracy: 96.34%\n",
      "\n",
      "Resultados:\n",
      "   optimizer  learning_rate num_filters  fc_neurons  dropout_rate  \\\n",
      "7       Adam          0.001    [32, 64]  [256, 128]          0.50   \n",
      "6       Adam          0.001    [32, 64]  [256, 128]          0.25   \n",
      "4       Adam          0.001    [32, 64]   [128, 64]          0.25   \n",
      "3       Adam          0.001    [16, 32]  [256, 128]          0.50   \n",
      "0       Adam          0.001    [16, 32]   [128, 64]          0.25   \n",
      "2       Adam          0.001    [16, 32]  [256, 128]          0.25   \n",
      "5       Adam          0.001    [32, 64]   [128, 64]          0.50   \n",
      "1       Adam          0.001    [16, 32]   [128, 64]          0.50   \n",
      "8       Adam          0.010    [16, 32]   [128, 64]          0.25   \n",
      "9       Adam          0.010    [16, 32]   [128, 64]          0.50   \n",
      "15      Adam          0.010    [32, 64]  [256, 128]          0.50   \n",
      "14      Adam          0.010    [32, 64]  [256, 128]          0.25   \n",
      "29       SGD          0.010    [32, 64]   [128, 64]          0.50   \n",
      "28       SGD          0.010    [32, 64]   [128, 64]          0.25   \n",
      "13      Adam          0.010    [32, 64]   [128, 64]          0.50   \n",
      "31       SGD          0.010    [32, 64]  [256, 128]          0.50   \n",
      "30       SGD          0.010    [32, 64]  [256, 128]          0.25   \n",
      "26       SGD          0.010    [16, 32]  [256, 128]          0.25   \n",
      "27       SGD          0.010    [16, 32]  [256, 128]          0.50   \n",
      "25       SGD          0.010    [16, 32]   [128, 64]          0.50   \n",
      "24       SGD          0.010    [16, 32]   [128, 64]          0.25   \n",
      "23       SGD          0.001    [32, 64]  [256, 128]          0.50   \n",
      "21       SGD          0.001    [32, 64]   [128, 64]          0.50   \n",
      "22       SGD          0.001    [32, 64]  [256, 128]          0.25   \n",
      "20       SGD          0.001    [32, 64]   [128, 64]          0.25   \n",
      "19       SGD          0.001    [16, 32]  [256, 128]          0.50   \n",
      "17       SGD          0.001    [16, 32]   [128, 64]          0.50   \n",
      "18       SGD          0.001    [16, 32]  [256, 128]          0.25   \n",
      "16       SGD          0.001    [16, 32]   [128, 64]          0.25   \n",
      "12      Adam          0.010    [32, 64]   [128, 64]          0.25   \n",
      "11      Adam          0.010    [16, 32]  [256, 128]          0.50   \n",
      "10      Adam          0.010    [16, 32]  [256, 128]          0.25   \n",
      "\n",
      "    train_accuracy  test_accuracy  \n",
      "7         0.992867         0.9905  \n",
      "6         0.992767         0.9904  \n",
      "4         0.992783         0.9893  \n",
      "3         0.991617         0.9891  \n",
      "0         0.991533         0.9887  \n",
      "2         0.991783         0.9879  \n",
      "5         0.990050         0.9864  \n",
      "1         0.988000         0.9860  \n",
      "8         0.977500         0.9780  \n",
      "9         0.969450         0.9724  \n",
      "15        0.971800         0.9720  \n",
      "14        0.974800         0.9705  \n",
      "29        0.962583         0.9658  \n",
      "28        0.961767         0.9649  \n",
      "13        0.965750         0.9636  \n",
      "31        0.962883         0.9634  \n",
      "30        0.959783         0.9631  \n",
      "26        0.959517         0.9629  \n",
      "27        0.952633         0.9569  \n",
      "25        0.951333         0.9543  \n",
      "24        0.949400         0.9516  \n",
      "23        0.617583         0.6221  \n",
      "21        0.385783         0.3889  \n",
      "22        0.352800         0.3701  \n",
      "20        0.305483         0.3133  \n",
      "19        0.285567         0.2902  \n",
      "17        0.207883         0.2108  \n",
      "18        0.191633         0.1923  \n",
      "16        0.155450         0.1565  \n",
      "12        0.112367         0.1135  \n",
      "11        0.112367         0.1135  \n",
      "10        0.112367         0.1135  \n"
     ]
    }
   ],
   "source": [
    "# Se usa el mismo train_dataset y test_dataset del perceptron multicapa\n",
    "# Solo se usaran 3 epocas como menciono el profesor para que la ejecución sea mas rapida\n",
    "\n",
    "\n",
    "# Definición de una red convolucional flexible\n",
    "class FlexibleCNN(nn.Module):\n",
    "    def __init__(self, num_filters, fc_neurons, dropout_rate):\n",
    "        super(FlexibleCNN, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, num_filters[0], kernel_size=3, stride=1, padding=1),  # Convolución 1\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # MaxPooling 1\n",
    "            nn.Conv2d(num_filters[0], num_filters[1], kernel_size=3, stride=1, padding=1),  # Convolución 2\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # MaxPooling 2\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(num_filters[1] * 7 * 7, fc_neurons[0]),  # Capa totalmente conectada 1\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(fc_neurons[0], fc_neurons[1]),  # Capa totalmente conectada 2\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(fc_neurons[1], 10)  # Salida para 10 clases\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, train_loader, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Parámetros para la experimentación\n",
    "batch_size = 32  # Fijo\n",
    "num_filters_list = [[16, 32], [32, 64]]  # 2 configuraciones\n",
    "fc_neurons_list = [[128, 64], [256, 128]]  # Reducido a 2 configuraciones\n",
    "dropout_rates = [0.25, 0.5]  # 2 valores\n",
    "num_epochs = 3 \n",
    "optimizers = [optim.Adam, optim.SGD]  # 2 optimizadores\n",
    "learning_rates = [0.001, 0.01]  # 2 valores\n",
    "\n",
    "# Experimentación\n",
    "results = []\n",
    "\n",
    "for opt_func in optimizers:\n",
    "    for lr in learning_rates:\n",
    "        for num_filters in num_filters_list:\n",
    "            for fc_neurons in fc_neurons_list:\n",
    "                for dropout_rate in dropout_rates:\n",
    "                    print(f\"Testing: optimizer={opt_func.__name__}, lr={lr}, filters={num_filters}, fc_neurons={fc_neurons}, dropout={dropout_rate}\")\n",
    "                    \n",
    "                    # Crear modelo\n",
    "                    model = FlexibleCNN(num_filters, fc_neurons, dropout_rate)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt_func(model.parameters(), lr=lr)\n",
    "                    \n",
    "                    # Entrenar modelo\n",
    "                    train_model(model, train_loader, optimizer, criterion, num_epochs)\n",
    "                    \n",
    "                    # Evaluar modelo\n",
    "                    train_acc = evaluate_model(model, train_loader)\n",
    "                    test_acc = evaluate_model(model, test_loader)\n",
    "                    \n",
    "                    print(f\"Train Accuracy: {train_acc * 100:.2f}%, Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "                    \n",
    "                    # Guardar resultados\n",
    "                    results.append({\n",
    "                        'optimizer': opt_func.__name__,\n",
    "                        'learning_rate': lr,\n",
    "                        'num_filters': num_filters,\n",
    "                        'fc_neurons': fc_neurons,\n",
    "                        'dropout_rate': dropout_rate,\n",
    "                        'train_accuracy': train_acc,\n",
    "                        'test_accuracy': test_acc\n",
    "                    })\n",
    "\n",
    "# Guardar resultados en un DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='test_accuracy', ascending=False)\n",
    "\n",
    "# Mostrar los mejores resultados\n",
    "print(\"\\nResultados:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ae4b563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mejor Configuración:\n",
      "optimizer               Adam\n",
      "learning_rate          0.001\n",
      "num_filters         [32, 64]\n",
      "fc_neurons        [256, 128]\n",
      "dropout_rate             0.5\n",
      "train_accuracy      0.992867\n",
      "test_accuracy         0.9905\n",
      "Name: 7, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Seleccionar la mejor arquitectura\n",
    "best_result = results_df.iloc[0]  # La primera fila ya es la mejor por orden descendente\n",
    "print(\"\\nMejor Configuración:\")\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd05f2f6",
   "metadata": {},
   "source": [
    "## Conclusiones CNN\n",
    "\n",
    "## Mejor Arquitectura Seleccionada\n",
    "La arquitectura que obtuvo el mejor rendimiento en términos de precisión en el conjunto de prueba fue:\n",
    "\n",
    "- **Optimizador**: Adam\n",
    "- **Learning Rate**: 0.001\n",
    "- **Número de Filtros**: [32, 64]\n",
    "- **Neuronas en Capas Conectadas**: [256, 128]\n",
    "- **Tasa de Dropout**: 0.50\n",
    "- **Precisión en Entrenamiento**: 99.29%\n",
    "- **Precisión en Prueba**: 99.05%\n",
    "\n",
    "Esta configuración demostró un balance adecuado entre capacidad de aprendizaje y generalización. La tasa de dropout moderada (0.50) proporcionó una regularización eficiente, permitiendo un rendimiento óptimo en el conjunto de prueba.\n",
    "\n",
    "\n",
    "## Impacto de los Parámetros\n",
    "\n",
    "### Número de Filtros y Neuronas en Capas Conectadas\n",
    "- Las configuraciones con un mayor número de filtros en las capas convolucionales (**[32, 64]**) proporcionaron un mejor rendimiento. Este diseño permitió al modelo capturar características más complejas y relevantes del dataset.\n",
    "- En las capas totalmente conectadas, un mayor número de neuronas (**[256, 128]**) se asoció con un rendimiento superior en comparación con configuraciones más simples (**[128, 64]**), lo que indica que estas configuraciones son adecuadas para aprovechar los patrones complejos aprendidos por las capas convolucionales.\n",
    "\n",
    "### Tasa de Dropout\n",
    "- Tasas de dropout más altas (**0.50**) mostraron un rendimiento competitivo, funcionando como un mecanismo efectivo para prevenir el sobreajuste en configuraciones más complejas.\n",
    "- Tasas más bajas (**0.25**) también lograron buenos resultados, pero en algunas configuraciones llevaron a un menor desempeño debido al riesgo de sobreajuste.\n",
    "\n",
    "### Optimizador\n",
    "- **Adam** fue consistentemente superior en términos de precisión tanto en entrenamiento como en prueba. Su capacidad para adaptar dinámicamente las tasas de aprendizaje lo convierte en una opción ideal para tareas como esta.\n",
    "- **SGD** mostró un rendimiento más variable. Si bien algunas configuraciones lograron precisión competitiva, otras no convergieron adecuadamente, especialmente con tasas de aprendizaje bajas.\n",
    "\n",
    "### Learning Rate\n",
    "- **Learning rate** de **0.001** fue más confiable y estable, logrando el mejor rendimiento en la mayoría de las configuraciones.\n",
    "- **Learning rate** de **0.01** condujo a resultados menos estables en algunas configuraciones, con una tendencia al sobreajuste o falta de convergencia.\n",
    "\n",
    "\n",
    "## Tendencias Generales\n",
    "\n",
    "- **Arquitecturas más profundas ofrecen mejores resultados**: El uso de configuraciones más complejas como **[32, 64]** para los filtros y **[256, 128]** para las neuronas totalmente conectadas resultó en un mejor rendimiento general.\n",
    "- **Optimizador Adam**: Adam fue significativamente mejor que SGD en este experimento, especialmente con una tasa de aprendizaje baja (**0.001**).\n",
    "- **Regularización efectiva con Dropout**: Tasas moderadas a altas de dropout (**0.50**) ofrecieron el mejor equilibrio entre aprendizaje y regularización.\n",
    "- **Tasa de Aprendizaje**: Una tasa de aprendizaje más baja (**0.001**) permitió al modelo alcanzar un rendimiento óptimo de manera estable.\n",
    "\n",
    "\n",
    "## Conclusiónes finales CNN\n",
    "\n",
    "La mejor arquitectura y configuración identificada combina un balance entre complejidad, regularización y generalización. Específicamente:\n",
    "\n",
    "- **Número de Filtros**: [32, 64]\n",
    "- **Neuronas en Capas Conectadas**: [256, 128]\n",
    "- **Tasa de Dropout**: 0.50\n",
    "- **Optimizador**: Adam\n",
    "- **Learning Rate**: 0.001\n",
    "\n",
    "Esta configuración puede ser recomendada como la más eficiente y robusta para clasificar imágenes en el conjunto de datos **MNIST**, destacándose tanto en precisión de entrenamiento como en generalización en el conjunto de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3e726",
   "metadata": {},
   "source": [
    "# 1.3 Comparación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02db1c61",
   "metadata": {},
   "source": [
    "comparación entre MLP y CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f65a23d",
   "metadata": {},
   "source": [
    "## **Conclusión Comparativa: MLP vs CNN (Perceptron multicapa vs redes convolucionales)**\n",
    "\n",
    "A continuación, se presenta una comparación detallada entre las dos arquitecturas implementadas: el **Perceptrón Multicapa (MLP)** y la **Red Neuronal Convolucional (CNN)**. La comparación considera los aspectos de rendimiento, número de parámetros y tiempo de ejecución.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Rendimiento**\n",
    "\n",
    "#### **MLP**\n",
    "- **Mejor precisión de prueba:** 96.98% (configuración: `[256, 128]`, activación `Sigmoid`, `Adam`, `batch_size=32`, `learning_rate=0.001`).\n",
    "- En configuraciones con `ReLU` como función de activación, las redes MLP mostraron una precisión ligeramente inferior (~96.43%) comparada con `Sigmoid`.\n",
    "- **Limitaciones:** El MLP presentó dificultades para aprender eficientemente cuando el optimizador era `SGD` o la tasa de aprendizaje era alta (`0.01`), con resultados inconsistentes.\n",
    "\n",
    "#### **CNN**\n",
    "- **Mejor precisión de prueba:** 99.05% (configuración: `[32, 64]` filtros, `[256, 128]` neuronas, `dropout=0.50`, `Adam`, `learning_rate=0.001`).\n",
    "- Las CNN superaron consistentemente al MLP en todos los experimentos debido a su capacidad de extraer características espaciales.\n",
    "- **Estabilidad:** Las CNN mantuvieron un rendimiento alto y estable con diferentes configuraciones, incluso con optimizadores más simples como `SGD`.\n",
    "\n",
    "#### **Comparación de Rendimiento**\n",
    "- Las **CNN** superaron significativamente a las **MLP** en precisión de prueba, especialmente para configuraciones optimizadas.\n",
    "- Las CNN aprovecharon mejor el optimizador `Adam` y una tasa de aprendizaje baja (`0.001`), logrando hasta un **2% más de precisión** en el conjunto de prueba.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Número de Parámetros**\n",
    "\n",
    "#### **MLP**\n",
    "- Los parámetros de MLP dependen principalmente del número de neuronas en las capas ocultas.\n",
    "- **Ejemplo de configuración óptima ([256, 128]):**\n",
    "  - Parámetros: \\( (784 \\times 256) + (256 \\times 128) + (128 \\times 10) = 218,378 \\).\n",
    "\n",
    "#### **CNN**\n",
    "- Los parámetros de las CNN son controlados por el número de filtros en las capas convolucionales y las neuronas en las capas totalmente conectadas.\n",
    "- **Ejemplo de configuración óptima ([32, 64] filtros, [256, 128] neuronas):**\n",
    "  - Parámetros convolucionales:\n",
    "    - Primera capa: \\( (3 \\times 3 \\times 1 \\times 32) + 32 = 320 \\).\n",
    "    - Segunda capa: \\( (3 \\times 3 \\times 32 \\times 64) + 64 = 18,496 \\).\n",
    "  - Parámetros en capas densas:\n",
    "    - Primera capa: \\( (7 \\times 7 \\times 64) \\times 256 = 802,816 \\).\n",
    "    - Segunda capa: \\( 256 \\times 128 = 32,768 \\).\n",
    "    - Salida: \\( 128 \\times 10 = 1,280 \\).\n",
    "  - **Total:** \\( 855,680 \\).\n",
    "\n",
    "#### **Comparación de Parámetros**\n",
    "- **MLP:** Es más ligero en términos de parámetros (~218,378 en configuración óptima).\n",
    "- **CNN:** Tiene más parámetros (~855,680 en configuración óptima), pero esta complejidad adicional es necesaria para modelar características espaciales.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Tiempo de Ejecución**\n",
    "\n",
    "#### **MLP**\n",
    "- Entrenamiento más rápido debido a su arquitectura simple y menor cantidad de parámetros.\n",
    "- **Limitaciones:** La eficiencia decrece en configuraciones con muchas capas ocultas o cuando se usan funciones de activación que ralentizan el cálculo como `Sigmoid`.\n",
    "\n",
    "#### **CNN**\n",
    "- Entrenamiento más lento debido a la presencia de operaciones convolucionales y un mayor número de parámetros.\n",
    "- Sin embargo, el entrenamiento con configuraciones optimizadas (como `ReLU` con `Adam`) permitió alcanzar alta precisión en un tiempo razonable.\n",
    "\n",
    "#### **Comparación de Tiempo**\n",
    "- El MLP tiene ventaja en términos de velocidad, pero las CNN logran mejor precisión a costa de un mayor tiempo de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conclusiones Finales**\n",
    "\n",
    "1. **Rendimiento General:**\n",
    "   - Las CNN son significativamente superiores al MLP en precisión, especialmente en configuraciones con optimizador `Adam` y una tasa de aprendizaje de `0.001`.\n",
    "\n",
    "2. **Eficiencia de Parámetros:**\n",
    "   - El MLP requiere menos parámetros, lo que lo hace adecuado para dispositivos con recursos limitados.\n",
    "   - Las CNN, aunque más complejas, justifican su mayor cantidad de parámetros con un rendimiento superior.\n",
    "\n",
    "3. **Tiempo de Ejecución:**\n",
    "   - El MLP es más rápido de entrenar, pero las CNN ofrecen mejores resultados con un tiempo de ejecución ligeramente mayor.\n",
    "\n",
    "4. **Recomendaciónes:**\n",
    "   - **Uso de MLP:** En problemas con conjuntos de datos pequeños, donde no hay una estructura espacial fuerte y los recursos computacionales son limitados.\n",
    "   - **Uso de CNN:** En tareas donde la estructura espacial (como imágenes) es crítica y se busca el mejor rendimiento.\n",
    "\n",
    "En resumen, las CNN son la mejor opción para el dataset MNIST debido a su capacidad de generalización y rendimiento superior. Sin embargo, el MLP sigue siendo una solución válida para aplicaciones rápidas y ligeras.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
