{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6f8412c",
   "metadata": {},
   "source": [
    "# Actividad 5 No evaluada Percepton Multicapa\n",
    "\n",
    "- **Profesor:** Francisco Perez Galarce\n",
    "- **Ayudante:** Yesenia Salinas\n",
    "\n",
    "**Integrante:**\n",
    "- Jorge Troncoso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5969dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se importan las librerias a utilizar\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efc070b",
   "metadata": {},
   "source": [
    "# 1.1 Perceptron multicapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9502da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento y carga de datos de MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                 # Convertimos imágenes a tensores\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalizamos a media 0 y varianza 1\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Crear dataloaders\n",
    "batch_size = 64  # Ajusta según la capacidad de GPU/CPU\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cde384",
   "metadata": {},
   "source": [
    "En el siguiente bloque de codigo se hara tanto la modificación de arquitectura como las modifaciones de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298be15d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3447279696742694\n",
      "Epoch [2/3], Loss: 0.1699446654121081\n",
      "Epoch [3/3], Loss: 0.12821243075355887\n",
      "Train Accuracy: 97.03%, Test Accuracy: 96.43%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.5335675369898478\n",
      "Epoch [2/3], Loss: 0.7393601686636607\n",
      "Epoch [3/3], Loss: 0.538245778520902\n",
      "Train Accuracy: 87.52%, Test Accuracy: 88.01%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.4049938795487086\n",
      "Epoch [2/3], Loss: 0.1965350634942452\n",
      "Epoch [3/3], Loss: 0.14589538647284112\n",
      "Train Accuracy: 96.79%, Test Accuracy: 96.22%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.217266143544515\n",
      "Epoch [2/3], Loss: 2.0094561297098794\n",
      "Epoch [3/3], Loss: 1.7560285143534342\n",
      "Train Accuracy: 71.45%, Test Accuracy: 72.86%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3079059141258399\n",
      "Epoch [2/3], Loss: 0.14796543912688892\n",
      "Epoch [3/3], Loss: 0.10981847726752361\n",
      "Train Accuracy: 96.81%, Test Accuracy: 96.30%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.0568165318171183\n",
      "Epoch [2/3], Loss: 1.1895438321431477\n",
      "Epoch [3/3], Loss: 0.6761494761943817\n",
      "Train Accuracy: 85.61%, Test Accuracy: 86.32%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.40494417422910534\n",
      "Epoch [2/3], Loss: 0.15746312631318968\n",
      "Epoch [3/3], Loss: 0.11454615754137437\n",
      "Train Accuracy: 97.56%, Test Accuracy: 96.98%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.304174777984619\n",
      "Epoch [2/3], Loss: 2.2959259001413983\n",
      "Epoch [3/3], Loss: 2.2906442822774253\n",
      "Train Accuracy: 11.33%, Test Accuracy: 11.48%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3035767988617222\n",
      "Epoch [2/3], Loss: 0.1487840723304699\n",
      "Epoch [3/3], Loss: 0.11495558643589417\n",
      "Train Accuracy: 97.28%, Test Accuracy: 96.73%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2560335353851317\n",
      "Epoch [2/3], Loss: 1.9857461044947307\n",
      "Epoch [3/3], Loss: 1.1750160808563233\n",
      "Train Accuracy: 79.30%, Test Accuracy: 79.88%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.42746190211474894\n",
      "Epoch [2/3], Loss: 0.16158872569153707\n",
      "Epoch [3/3], Loss: 0.1225930577300489\n",
      "Train Accuracy: 96.80%, Test Accuracy: 96.10%\n",
      "Testing: batch_size=32, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3067314741770426\n",
      "Epoch [2/3], Loss: 2.301244281133016\n",
      "Epoch [3/3], Loss: 2.301096531041463\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.42248622870743274\n",
      "Epoch [2/3], Loss: 0.34188596717814607\n",
      "Epoch [3/3], Loss: 0.32211744655023017\n",
      "Train Accuracy: 92.81%, Test Accuracy: 92.74%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.5700919307669003\n",
      "Epoch [2/3], Loss: 0.31115918449958163\n",
      "Epoch [3/3], Loss: 0.2640207984258731\n",
      "Train Accuracy: 92.96%, Test Accuracy: 92.79%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.5550715933005015\n",
      "Epoch [2/3], Loss: 0.48210645522276563\n",
      "Epoch [3/3], Loss: 0.45514814260403313\n",
      "Train Accuracy: 85.51%, Test Accuracy: 85.94%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.3087553638617198\n",
      "Epoch [2/3], Loss: 0.5536598283052444\n",
      "Epoch [3/3], Loss: 0.42357916610240937\n",
      "Train Accuracy: 89.44%, Test Accuracy: 89.96%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.44225204476118085\n",
      "Epoch [2/3], Loss: 0.3284375795776645\n",
      "Epoch [3/3], Loss: 0.31304923750211794\n",
      "Train Accuracy: 90.48%, Test Accuracy: 90.86%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.6915874194562435\n",
      "Epoch [2/3], Loss: 0.3092312136809031\n",
      "Epoch [3/3], Loss: 0.2532151914099852\n",
      "Train Accuracy: 93.40%, Test Accuracy: 93.44%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.7365374496459961\n",
      "Epoch [2/3], Loss: 0.5206813677469889\n",
      "Epoch [3/3], Loss: 0.47621908397475876\n",
      "Train Accuracy: 87.36%, Test Accuracy: 87.09%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2729768065134683\n",
      "Epoch [2/3], Loss: 1.9613723939259846\n",
      "Epoch [3/3], Loss: 1.1898291874885558\n",
      "Train Accuracy: 73.93%, Test Accuracy: 74.30%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.48652776128252345\n",
      "Epoch [2/3], Loss: 0.37396025804479915\n",
      "Epoch [3/3], Loss: 0.3302409612228473\n",
      "Train Accuracy: 91.46%, Test Accuracy: 91.38%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.8895409260233244\n",
      "Epoch [2/3], Loss: 0.31659157854914666\n",
      "Epoch [3/3], Loss: 0.2443584785014391\n",
      "Train Accuracy: 93.67%, Test Accuracy: 93.63%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 2.3033239120483397\n",
      "Epoch [2/3], Loss: 2.3026454249064128\n",
      "Epoch [3/3], Loss: 2.3030910783131917\n",
      "Train Accuracy: 9.93%, Test Accuracy: 10.32%\n",
      "Testing: batch_size=32, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3035496841430665\n",
      "Epoch [2/3], Loss: 2.301682694498698\n",
      "Epoch [3/3], Loss: 2.2993578046162924\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.38540839714440966\n",
      "Epoch [2/3], Loss: 0.2007106315277056\n",
      "Epoch [3/3], Loss: 0.14557328371247694\n",
      "Train Accuracy: 95.81%, Test Accuracy: 95.53%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.9116417595318385\n",
      "Epoch [2/3], Loss: 1.1937532811276694\n",
      "Epoch [3/3], Loss: 0.8201434551271548\n",
      "Train Accuracy: 84.25%, Test Accuracy: 85.02%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.48050248352036296\n",
      "Epoch [2/3], Loss: 0.2251380700062015\n",
      "Epoch [3/3], Loss: 0.17178049441307847\n",
      "Train Accuracy: 95.80%, Test Accuracy: 95.52%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.258503119066072\n",
      "Epoch [2/3], Loss: 2.1693951892954453\n",
      "Epoch [3/3], Loss: 2.073139704112559\n",
      "Train Accuracy: 61.42%, Test Accuracy: 62.88%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.3375118201229173\n",
      "Epoch [2/3], Loss: 0.14655184533907725\n",
      "Epoch [3/3], Loss: 0.10850734239134358\n",
      "Train Accuracy: 97.04%, Test Accuracy: 96.42%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2135461492579123\n",
      "Epoch [2/3], Loss: 1.9209799033238182\n",
      "Epoch [3/3], Loss: 1.3997930284502156\n",
      "Train Accuracy: 77.11%, Test Accuracy: 78.03%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.49200111847601213\n",
      "Epoch [2/3], Loss: 0.17491909791864374\n",
      "Epoch [3/3], Loss: 0.12218713255396593\n",
      "Train Accuracy: 97.00%, Test Accuracy: 96.22%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3036901035796857\n",
      "Epoch [2/3], Loss: 2.2974442952731526\n",
      "Epoch [3/3], Loss: 2.295168396506482\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.33267770994748513\n",
      "Epoch [2/3], Loss: 0.14468885847786342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Loss: 0.11270095981401342\n",
      "Train Accuracy: 97.17%, Test Accuracy: 96.65%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.281759505587092\n",
      "Epoch [2/3], Loss: 2.2175865320762846\n",
      "Epoch [3/3], Loss: 2.0855780517114506\n",
      "Train Accuracy: 54.45%, Test Accuracy: 55.26%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.5071390755295849\n",
      "Epoch [2/3], Loss: 0.1672256277925742\n",
      "Epoch [3/3], Loss: 0.11762746465283194\n",
      "Train Accuracy: 97.33%, Test Accuracy: 96.54%\n",
      "Testing: batch_size=64, lr=0.001, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3065902208214375\n",
      "Epoch [2/3], Loss: 2.3011423293461424\n",
      "Epoch [3/3], Loss: 2.301063333493052\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.4244051267152656\n",
      "Epoch [2/3], Loss: 0.27683216802426364\n",
      "Epoch [3/3], Loss: 0.25225650303677394\n",
      "Train Accuracy: 92.81%, Test Accuracy: 92.40%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.7502501797892137\n",
      "Epoch [2/3], Loss: 0.36960331231419213\n",
      "Epoch [3/3], Loss: 0.32315585802771896\n",
      "Train Accuracy: 91.10%, Test Accuracy: 91.33%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.4810974602220155\n",
      "Epoch [2/3], Loss: 0.3666122185824904\n",
      "Epoch [3/3], Loss: 0.3308877024426262\n",
      "Train Accuracy: 89.65%, Test Accuracy: 89.83%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.7651780947947553\n",
      "Epoch [2/3], Loss: 0.9039194655062547\n",
      "Epoch [3/3], Loss: 0.6151133805576926\n",
      "Train Accuracy: 87.13%, Test Accuracy: 87.72%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.40334341323563155\n",
      "Epoch [2/3], Loss: 0.26744142689430384\n",
      "Epoch [3/3], Loss: 0.2539779929412422\n",
      "Train Accuracy: 91.32%, Test Accuracy: 91.07%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 0.9812655999366917\n",
      "Epoch [2/3], Loss: 0.3786165745082949\n",
      "Epoch [3/3], Loss: 0.31947228032102715\n",
      "Train Accuracy: 91.14%, Test Accuracy: 91.50%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.614411215045686\n",
      "Epoch [2/3], Loss: 0.3999735041023063\n",
      "Epoch [3/3], Loss: 0.35762457961021965\n",
      "Train Accuracy: 87.40%, Test Accuracy: 87.57%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.2922369861907796\n",
      "Epoch [2/3], Loss: 2.2559181202703447\n",
      "Epoch [3/3], Loss: 2.146525289839519\n",
      "Train Accuracy: 43.48%, Test Accuracy: 44.21%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=Adam\n",
      "Epoch [1/3], Loss: 0.49230037686397143\n",
      "Epoch [2/3], Loss: 0.2633975143911742\n",
      "Epoch [3/3], Loss: 0.24073136519235588\n",
      "Train Accuracy: 94.89%, Test Accuracy: 94.23%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=ReLU, optimizer=SGD\n",
      "Epoch [1/3], Loss: 1.3506683846717196\n",
      "Epoch [2/3], Loss: 0.4123342086447836\n",
      "Epoch [3/3], Loss: 0.3282964086529416\n",
      "Train Accuracy: 91.33%, Test Accuracy: 91.55%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=Adam\n",
      "Epoch [1/3], Loss: 1.0292762904596735\n",
      "Epoch [2/3], Loss: 0.5731181918716888\n",
      "Epoch [3/3], Loss: 0.5178883922284346\n",
      "Train Accuracy: 80.44%, Test Accuracy: 80.94%\n",
      "Testing: batch_size=64, lr=0.01, hidden_sizes=[512, 256, 128], activation=Sigmoid, optimizer=SGD\n",
      "Epoch [1/3], Loss: 2.3025384561847777\n",
      "Epoch [2/3], Loss: 2.3014930662061612\n",
      "Epoch [3/3], Loss: 2.3006053090349696\n",
      "Train Accuracy: 11.24%, Test Accuracy: 11.35%\n",
      "    batch_size  learning_rate    hidden_layers activation optimizer  \\\n",
      "6           32          0.001       [256, 128]    Sigmoid      Adam   \n",
      "8           32          0.001  [512, 256, 128]       ReLU      Adam   \n",
      "32          64          0.001  [512, 256, 128]       ReLU      Adam   \n",
      "34          64          0.001  [512, 256, 128]    Sigmoid      Adam   \n",
      "0           32          0.001            [128]       ReLU      Adam   \n",
      "28          64          0.001       [256, 128]       ReLU      Adam   \n",
      "4           32          0.001       [256, 128]       ReLU      Adam   \n",
      "2           32          0.001            [128]    Sigmoid      Adam   \n",
      "30          64          0.001       [256, 128]    Sigmoid      Adam   \n",
      "10          32          0.001  [512, 256, 128]    Sigmoid      Adam   \n",
      "24          64          0.001            [128]       ReLU      Adam   \n",
      "26          64          0.001            [128]    Sigmoid      Adam   \n",
      "44          64          0.010  [512, 256, 128]       ReLU      Adam   \n",
      "21          32          0.010  [512, 256, 128]       ReLU       SGD   \n",
      "17          32          0.010       [256, 128]       ReLU       SGD   \n",
      "13          32          0.010            [128]       ReLU       SGD   \n",
      "12          32          0.010            [128]       ReLU      Adam   \n",
      "36          64          0.010            [128]       ReLU      Adam   \n",
      "45          64          0.010  [512, 256, 128]       ReLU       SGD   \n",
      "41          64          0.010       [256, 128]       ReLU       SGD   \n",
      "20          32          0.010  [512, 256, 128]       ReLU      Adam   \n",
      "37          64          0.010            [128]       ReLU       SGD   \n",
      "40          64          0.010       [256, 128]       ReLU      Adam   \n",
      "16          32          0.010       [256, 128]       ReLU      Adam   \n",
      "15          32          0.010            [128]    Sigmoid       SGD   \n",
      "38          64          0.010            [128]    Sigmoid      Adam   \n",
      "1           32          0.001            [128]       ReLU       SGD   \n",
      "39          64          0.010            [128]    Sigmoid       SGD   \n",
      "42          64          0.010       [256, 128]    Sigmoid      Adam   \n",
      "18          32          0.010       [256, 128]    Sigmoid      Adam   \n",
      "5           32          0.001       [256, 128]       ReLU       SGD   \n",
      "14          32          0.010            [128]    Sigmoid      Adam   \n",
      "25          64          0.001            [128]       ReLU       SGD   \n",
      "46          64          0.010  [512, 256, 128]    Sigmoid      Adam   \n",
      "9           32          0.001  [512, 256, 128]       ReLU       SGD   \n",
      "29          64          0.001       [256, 128]       ReLU       SGD   \n",
      "19          32          0.010       [256, 128]    Sigmoid       SGD   \n",
      "3           32          0.001            [128]    Sigmoid       SGD   \n",
      "27          64          0.001            [128]    Sigmoid       SGD   \n",
      "33          64          0.001  [512, 256, 128]       ReLU       SGD   \n",
      "43          64          0.010       [256, 128]    Sigmoid       SGD   \n",
      "7           32          0.001       [256, 128]    Sigmoid       SGD   \n",
      "11          32          0.001  [512, 256, 128]    Sigmoid       SGD   \n",
      "35          64          0.001  [512, 256, 128]    Sigmoid       SGD   \n",
      "31          64          0.001       [256, 128]    Sigmoid       SGD   \n",
      "23          32          0.010  [512, 256, 128]    Sigmoid       SGD   \n",
      "47          64          0.010  [512, 256, 128]    Sigmoid       SGD   \n",
      "22          32          0.010  [512, 256, 128]    Sigmoid      Adam   \n",
      "\n",
      "    train_accuracy  test_accuracy  \n",
      "6         0.975600         0.9698  \n",
      "8         0.972783         0.9673  \n",
      "32        0.971750         0.9665  \n",
      "34        0.973333         0.9654  \n",
      "0         0.970333         0.9643  \n",
      "28        0.970433         0.9642  \n",
      "4         0.968100         0.9630  \n",
      "2         0.967867         0.9622  \n",
      "30        0.970017         0.9622  \n",
      "10        0.968050         0.9610  \n",
      "24        0.958150         0.9553  \n",
      "26        0.957950         0.9552  \n",
      "44        0.948917         0.9423  \n",
      "21        0.936700         0.9363  \n",
      "17        0.934000         0.9344  \n",
      "13        0.929567         0.9279  \n",
      "12        0.928083         0.9274  \n",
      "36        0.928100         0.9240  \n",
      "45        0.913267         0.9155  \n",
      "41        0.911450         0.9150  \n",
      "20        0.914567         0.9138  \n",
      "37        0.910967         0.9133  \n",
      "40        0.913167         0.9107  \n",
      "16        0.904783         0.9086  \n",
      "15        0.894383         0.8996  \n",
      "38        0.896500         0.8983  \n",
      "1         0.875167         0.8801  \n",
      "39        0.871317         0.8772  \n",
      "42        0.874017         0.8757  \n",
      "18        0.873617         0.8709  \n",
      "5         0.856067         0.8632  \n",
      "14        0.855117         0.8594  \n",
      "25        0.842533         0.8502  \n",
      "46        0.804350         0.8094  \n",
      "9         0.793033         0.7988  \n",
      "29        0.771067         0.7803  \n",
      "19        0.739283         0.7430  \n",
      "3         0.714500         0.7286  \n",
      "27        0.614167         0.6288  \n",
      "33        0.544517         0.5526  \n",
      "43        0.434750         0.4421  \n",
      "7         0.113267         0.1148  \n",
      "11        0.112367         0.1135  \n",
      "35        0.112367         0.1135  \n",
      "31        0.112367         0.1135  \n",
      "23        0.112367         0.1135  \n",
      "47        0.112367         0.1135  \n",
      "22        0.099300         0.1032  \n"
     ]
    }
   ],
   "source": [
    "# Función para construir un MLP flexible\n",
    "# Solo se usaran 3 epocas como menciono el profesor para que la ejecución sea mas rapida\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activation_function):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        current_size = input_size\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(current_size, hidden_size))\n",
    "            layers.append(activation_function())\n",
    "            current_size = hidden_size\n",
    "        layers.append(nn.Linear(current_size, output_size))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Aplanar las imágenes\n",
    "        return self.network(x)\n",
    "\n",
    "# Función para entrenar el modelo\n",
    "def train_model(model, train_loader, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Función para evaluar el modelo\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total\n",
    "\n",
    "# Experimentación\n",
    "results = []\n",
    "batch_sizes = [32, 64]\n",
    "learning_rates = [0.001, 0.01]\n",
    "hidden_layers = [[128], [256, 128], [512, 256, 128]]\n",
    "activation_functions = [nn.ReLU, nn.Sigmoid]\n",
    "optimizers = [optim.Adam, optim.SGD]\n",
    "\n",
    "input_size = 28 * 28  # Tamaño de entrada\n",
    "output_size = 10  # 10 clases de dígitos\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for lr in learning_rates:\n",
    "        for hidden_sizes in hidden_layers:\n",
    "            for activation_function in activation_functions:\n",
    "                for opt_func in optimizers:\n",
    "                    print(f\"Testing: batch_size={batch_size}, lr={lr}, hidden_sizes={hidden_sizes}, activation={activation_function.__name__}, optimizer={opt_func.__name__}\")\n",
    "                    # Crear modelo\n",
    "                    model = MLP(input_size, hidden_sizes, output_size, activation_function)\n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = opt_func(model.parameters(), lr=lr)\n",
    "                    # Entrenar modelo\n",
    "                    train_model(model, train_loader, optimizer, criterion, num_epochs=3)\n",
    "                    # Evaluar modelo\n",
    "                    train_acc = evaluate_model(model, train_loader)\n",
    "                    test_acc = evaluate_model(model, test_loader)\n",
    "                    print(f\"Train Accuracy: {train_acc * 100:.2f}%, Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "                    # Guardar resultados\n",
    "                    results.append({\n",
    "                        'batch_size': batch_size,\n",
    "                        'learning_rate': lr,\n",
    "                        'hidden_layers': hidden_sizes,\n",
    "                        'activation': activation_function.__name__,\n",
    "                        'optimizer': opt_func.__name__,\n",
    "                        'train_accuracy': train_acc,\n",
    "                        'test_accuracy': test_acc\n",
    "                    })\n",
    "\n",
    "# Mostrar resultados ordenados por mejor accuracy en test\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values(by='test_accuracy', ascending=False)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf195f3e",
   "metadata": {},
   "source": [
    "## **Informe Detallado de los Experimentos** (Tanto de la arquitectura como del entrenamiento)\n",
    "\n",
    "## **Objetivo de los Experimentos**\n",
    "El propósito de estos experimentos fue evaluar cómo diferentes configuraciones arquitectónicas y estrategias de entrenamiento impactan el desempeño de un Perceptrón Multicapa (MLP) entrenado para clasificar dígitos del dataset MNIST. El análisis se llevó a cabo con dos enfoques principales:\n",
    "\n",
    "1. **Arquitectura del Modelo:**\n",
    "   - Variar el número de capas ocultas y neuronas.\n",
    "   - Cambiar la función de activación.\n",
    "2. **Estrategia de Entrenamiento:**\n",
    "   - Probar diferentes optimizadores y tasas de aprendizaje.\n",
    "   - Evaluar el impacto del tamaño del batch.\n",
    "\n",
    "---\n",
    "\n",
    "## **Parte 1: Modificaciones en la Arquitectura**\n",
    "\n",
    "### **Configuraciones Probadas**\n",
    "Se probaron 3 configuraciones de capas ocultas: \n",
    "- `[128]`\n",
    "- `[256, 128]`\n",
    "- `[512, 256, 128]`\n",
    "\n",
    "Cada configuración se probó con dos funciones de activación: \n",
    "- `ReLU` (Rectified Linear Unit).\n",
    "- `Sigmoid`.\n",
    "\n",
    "### **Resultados Resumidos**\n",
    "\n",
    "| Arquitectura         | Activación | Optimizador | Train Accuracy | Test Accuracy | Observaciones                                          |\n",
    "|----------------------|------------|-------------|----------------|---------------|-------------------------------------------------------|\n",
    "| `[256, 128]`         | `Sigmoid`  | `Adam`      | 97.56%         | **96.98%**    | Excelente generalización con configuraciones intermedias. |\n",
    "| `[512, 256, 128]`    | `ReLU`     | `Adam`      | 97.28%         | 96.73%        | Profundidad proporciona generalización sólida.         |\n",
    "| `[128]`              | `ReLU`     | `Adam`      | 97.03%         | 96.43%        | Arquitectura más simple con buen rendimiento.          |\n",
    "| `[256, 128]`         | `ReLU`     | `Adam`      | 96.81%         | 96.30%        | Buen rendimiento general, menor que configuraciones más profundas. |\n",
    "| `[128]`              | `Sigmoid`  | `Adam`      | 96.79%         | 96.22%        | `Sigmoid` limita el rendimiento en configuraciones simples. |\n",
    "| `[512, 256, 128]`    | `Sigmoid`  | `Adam`      | 96.80%         | 96.10%        | Menor precisión comparada con `ReLU`.                  |\n",
    "\n",
    "### **Análisis y Conclusión**\n",
    "\n",
    "1. **Función de Activación:**\n",
    "   - `ReLU` mostró mejor desempeño en general, especialmente en configuraciones profundas.\n",
    "   - `Sigmoid` tiene problemas con arquitecturas más complejas debido al gradiente desvanecido.\n",
    "\n",
    "2. **Arquitectura:**\n",
    "   - Las arquitecturas intermedias (`[256, 128]`) logran el mejor equilibrio entre rendimiento y costo computacional.\n",
    "   - Redes más profundas como `[512, 256, 128]` ofrecen mayor capacidad de modelado pero requieren más tiempo de entrenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## **Parte 2: Modificaciones en el Entrenamiento**\n",
    "\n",
    "### **Configuraciones Probadas**\n",
    "- **Tamaño del batch:** `32` y `64`.\n",
    "- **Learning rate (lr):** `0.001` y `0.01`.\n",
    "- **Optimizadores:** `Adam` y `SGD`.\n",
    "\n",
    "### **Resultados Resumidos**\n",
    "\n",
    "| Batch Size | LR    | Optimizador | Train Accuracy | Test Accuracy | Observaciones                                    |\n",
    "|------------|-------|-------------|----------------|---------------|-------------------------------------------------|\n",
    "| `32`       | 0.001 | `Adam`      | 97.56%         | **96.98%**    | Estabilidad y buena generalización.             |\n",
    "| `64`       | 0.001 | `Adam`      | 97.33%         | 96.54%        | Precisión sólida con menor costo computacional. |\n",
    "| `32`       | 0.001 | `SGD`       | 87.52%         | 88.01%        | Lento en converger, menor precisión general.    |\n",
    "| `32`       | 0.01  | `Adam`      | 92.81%         | 92.74%        | Learning rate alto afecta la generalización.    |\n",
    "| `64`       | 0.01  | `SGD`       | 91.14%         | 91.50%        | `SGD` mejora con menor batch size.              |\n",
    "\n",
    "### **Análisis y Conclusión**\n",
    "\n",
    "1. **Tasa de Aprendizaje:**\n",
    "   - `lr=0.001` fue óptimo, especialmente con `Adam`.\n",
    "   - `lr=0.01` mostró menor estabilidad en la generalización.\n",
    "\n",
    "2. **Optimizador:**\n",
    "   - `Adam` superó consistentemente a `SGD` en precisión y convergencia.\n",
    "   - `SGD` es competitivo con tasas de aprendizaje adecuadas pero requiere más ajuste fino.\n",
    "\n",
    "3. **Tamaño del Batch:**\n",
    "   - Batch size de `32` proporcionó mejor generalización que `64`, aunque a mayor costo computacional.\n",
    "\n",
    "---\n",
    "\n",
    "## **Selección del Modelo Final**\n",
    "\n",
    "### **Modelo Seleccionado:**\n",
    "- **Arquitectura:** `[256, 128]` con `Sigmoid`.\n",
    "- **Entrenamiento:**\n",
    "  - **Batch size:** `32`.\n",
    "  - **Learning rate:** `0.001`.\n",
    "  - **Optimizador:** `Adam`.\n",
    "\n",
    "### **Rendimiento:**\n",
    "- **Train Accuracy:** 97.56%.\n",
    "- **Test Accuracy:** 96.98%.\n",
    "\n",
    "### **Justificación:**\n",
    "- La configuración seleccionada equilibra profundidad y estabilidad de entrenamiento.\n",
    "- Proporciona el mejor desempeño en términos de precisión en pruebas.\n",
    "- Es computacionalmente eficiente.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusión General**\n",
    "\n",
    "1. **Arquitectura:**\n",
    "   - Las configuraciones intermedias (`[256, 128]`) mostraron ser ideales para este problema, combinando simplicidad y precisión.\n",
    "   - Configuraciones más simples (`[128]`) son útiles en escenarios con limitaciones computacionales.\n",
    "\n",
    "2. **Estrategia de Entrenamiento:**\n",
    "   - El optimizador `Adam` con `lr=0.001` y batch size de `32` fue la mejor combinación para alcanzar alta precisión.\n",
    "\n",
    "3. **Recomendaciones Futuras:**\n",
    "   - Extender el número de épocas para confirmar estabilidad en redes profundas.\n",
    "   - Probar técnicas de regularización como Dropout o Batch Normalization.\n",
    "   - Aplicar la configuración seleccionada en datasets más complejos para evaluar generalización.\n",
    "\n",
    "---\n",
    "\n",
    "## **Apreciaciones Extras**\n",
    "- **Impacto del Gradiente Desvanecido:** `Sigmoid` funciona bien en arquitecturas simples pero es menos eficiente en configuraciones profundas.\n",
    "- **Implicaciones Prácticas:** En dispositivos con restricciones, usar configuraciones simples como `[128]` puede ser una solución viable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
